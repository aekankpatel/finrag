
# ğŸ“Š FinRAG â€” Financial Retrieval-Augmented Intelligence System

A fully local RAG (Retrieval-Augmented Generation) system for querying and analyzing financial documents including SEC filings, earnings call transcripts, and macro research reports â€” with zero OpenAI API costs.

---

## ğŸ¯ What It Does

- Ask natural language questions across 25+ financial documents

- Get grounded answers with cited sources and relevance scores

- Auto-detects which company you're asking about

- Compare two companies side by side on the same question

- Fully local â€” runs on your machine using Ollama

---

## ğŸ—ï¸ Architecture

```

PDFs / Documents

      â†“

Text Extraction & Cleaning (ingest_pdfs.py)

      â†“

Chunking (TokenTextSplitter, 128 tokens)

      â†“

Embeddings (nomic-embed-text via Ollama)

      â†“

Vector Index (LlamaIndex SimpleVectorStore)

      â†“

User Query

      â†“

Semantic Retrieval (top-k chunks)

      â†“

LLM Generation (llama3.1:8b via Ollama)

      â†“

Answer + Source Citations

```

---

## ğŸ§° Tech Stack

| Component | Tool |

|---|---|

| LLM | llama3.1:8b (Ollama) |

| Embeddings | nomic-embed-text (Ollama) |

| Vector Store | LlamaIndex SimpleVectorStore |

| Document Parsing | LlamaIndex + custom ingestion |

| UI | Streamlit |

| Language | Python 3.13 |

---

## ğŸ“ Document Corpus (25 files)

- **SEC Filings**: Meta 10-K, Amazon 10-K/10-Q, NVIDIA 10-Q, Apple 10-K/10-Q, Tesla 10-K/10-Q, Microsoft 10-Q, Goldman Sachs BDC 10-Q, Bank of America Annual Report

- **Earnings Transcripts**: Amazon Q4, BofA Q4, Goldman Sachs Q4, JPMorgan Q4, NVIDIA Q4, Walmart Q4

- **Macro Research**: EY Global Banking Outlook 2025, Goldman Sachs 2026 M&A Outlook, World Bank Global Economic Prospects Jan 2026, Capital Markets Forecast 2026

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+

- [Ollama](https://ollama.com) installed and running

### 1. Clone the repo

```bash

git clone https://github.com/YOUR_USERNAME/finrag.git

cd finrag

```

### 2. Install dependencies

```bash

pip install llama-index llama-index-embeddings-ollama llama-index-llms-ollama streamlit tqdm faiss-cpu

```

### 3. Pull Ollama models

```bash

ollama pull llama3.1:8b

ollama pull nomic-embed-text

```

### 4. Add your documents

Place `.pdf` files in `finrag/data/raw/` then run:

```bash

python finrag/ingest_pdfs.py

```

### 5. Build the vector index

```bash

python finrag/build_index.py

```

### 6. Launch the UI

```bash

streamlit run finrag/app.py

```

---

## ğŸ–¥ï¸ Features

- **ğŸ” Auto-detect** â€” detects company name from your question automatically

- **âš–ï¸ Compare mode** â€” side-by-side answers from two different documents

- **ğŸ“Š Confidence bar** â€” visual retrieval confidence score per query

- **ğŸ’¬ Chat history** â€” scrollable Q&A log within the session

- **â¬‡ï¸ Export** â€” download any answer + sources as a `.txt` file

- **ğŸ›ï¸ Manual override** â€” filter to any specific document via dropdown

---

## ğŸ“Š Evaluation

Evaluated on 10 domain-specific financial questions across 5 companies:

| Metric | Result |

|---|---|

| Total questions | 10 |

| Pass rate (score â‰¥ 0.5) | 8/10 (80%) |

| Average keyword hit score | 0.57 / 1.0 |

| Avg response time | ~20 seconds |

| LLM | llama3.1:8b (fully local) |

Evaluation script: `finrag/evaluate.py`

Full results: `finrag/eval_results.json`

---

## ğŸ“‚ Project Structure

```

finrag/

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ raw/           # Original PDFs

â”‚   â””â”€â”€ processed/     # Cleaned .txt files

â”œâ”€â”€ index/             # Vector store (auto-generated)

â”œâ”€â”€ ingest_pdfs.py     # PDF â†’ text pipeline

â”œâ”€â”€ build_index.py     # Chunking + embedding + indexing

â”œâ”€â”€ query.py           # Terminal query interface

â”œâ”€â”€ app.py             # Streamlit web UI

â”œâ”€â”€ evaluate.py        # Evaluation script

â””â”€â”€ eval_results.json  # Evaluation output

```

---

## ğŸ’¼ Skills Demonstrated

- Document ingestion & preprocessing pipeline

- Semantic search with vector embeddings

- Retrieval-Augmented Generation (RAG)

- Local LLM inference (no API costs)

- Evaluation framework for LLM outputs

- Full-stack AI application with Streamlit UI

---

## ğŸ”® Future Work

- LLM-based query routing (replace keyword detection)

- Re-ranking retrieved chunks for better precision

- Multi-turn conversation memory

- Deployment to cloud (Streamlit Cloud / HuggingFace Spaces)

---

*Built with LlamaIndex, Ollama, and Streamlit. Runs fully locally.*

